{"nbformat_minor": 2, "cells": [{"execution_count": 1, "cell_type": "code", "source": "spark", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1517953339518_0005</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-bluewa.jvasazpi0emufe2iutidhl1p1g.bx.internal.cloudapp.net:8088/proxy/application_1517953339518_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.14:30060/node/containerlogs/container_1517953339518_0005_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n<pyspark.sql.session.SparkSession object at 0x7f179add0f98>"}], "metadata": {"collapsed": false}}, {"execution_count": 7, "cell_type": "code", "source": "%%configure -f\n{ \"jars\": [\"wasb:///jar/hwxhelloworld_2.11-1.0.jar\"] }", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1517953339518_0006</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-bluewa.jvasazpi0emufe2iutidhl1p1g.bx.internal.cloudapp.net:8088/proxy/application_1517953339518_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.14:30060/node/containerlogs/container_1517953339518_0006_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'jars': [u'wasb:///jar/hwxhelloworld_2.11-1.0.jar'], u'kind': 'pyspark3'}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1517953339518_0006</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-bluewa.jvasazpi0emufe2iutidhl1p1g.bx.internal.cloudapp.net:8088/proxy/application_1517953339518_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.14:30060/node/containerlogs/container_1517953339518_0006_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"execution_count": 8, "cell_type": "code", "source": "# SQL Database & Data Warehouse Location\nuser = \"sqladmin\"\npassword = \"Savannah!1979\"\ndatabaseHost = \"bluewatersql.database.windows.net\"\n\nif not user:\n  raise Exception(\"Please complete the Database-Settings notebook with the database login settings\")\n\njdbc_url_db = \"jdbc:sqlserver://{}:1433;database=AdventureWorksDB;user={};password={};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\".format(databaseHost, user, password)\njdbc_url_dw = \"jdbc:sqlserver://{}:1433;database=AdventureWorksDW;user={};password={};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\".format(databaseHost, user, password)\n\nNone # Suppress any output", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 9, "cell_type": "code", "source": "from pyspark.sql.functions import *\nfrom pyspark.sql.types import *", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 10, "cell_type": "code", "source": "existingProductCategoryDF = spark.read.jdbc(jdbc_url_db, \"SalesLT.ProductCategory\").filter(col(\"ProductCategoryId\") <= 100).orderBy(\"ProductCategoryId\")", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 5, "cell_type": "code", "source": "existingProductCategoryDF.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----------------+-----------------------+---------------+--------------------+--------------------+\n|ProductCategoryID|ParentProductCategoryID|           Name|             rowguid|        ModifiedDate|\n+-----------------+-----------------------+---------------+--------------------+--------------------+\n|                1|                   null|          Bikes|CFBDA25C-DF71-47A...|2002-06-01 00:00:...|\n|                2|                   null|     Components|C657828D-D808-4AB...|2002-06-01 00:00:...|\n|                3|                   null|       Clothing|10A7C342-CA82-48D...|2002-06-01 00:00:...|\n|                4|                   null|    Accessories|2BE3BE36-D9A2-4EE...|2002-06-01 00:00:...|\n|                5|                      1| Mountain Bikes|2D364ADE-264A-433...|2002-06-01 00:00:...|\n|                6|                      1|     Road Bikes|000310C0-BCC8-42C...|2002-06-01 00:00:...|\n|                7|                      1|  Touring Bikes|02C5061D-ECDC-427...|2002-06-01 00:00:...|\n|                8|                      2|     Handlebars|3EF2C725-7135-4C8...|2002-06-01 00:00:...|\n|                9|                      2|Bottom Brackets|A9E54089-8A1E-4CF...|2002-06-01 00:00:...|\n|               10|                      2|         Brakes|D43BA4A3-EF0D-426...|2002-06-01 00:00:...|\n|               11|                      2|         Chains|E93A7231-F16C-4B0...|2002-06-01 00:00:...|\n|               12|                      2|      Cranksets|4F644521-422B-4F1...|2002-06-01 00:00:...|\n|               13|                      2|    Derailleurs|1830D70C-AA2A-40C...|2002-06-01 00:00:...|\n|               14|                      2|          Forks|B5F9BA42-B69B-4FD...|2002-06-01 00:00:...|\n|               15|                      2|       Headsets|7C782BBE-5A16-495...|2002-06-01 00:00:...|\n|               16|                      2|Mountain Frames|61B21B65-E16A-4BE...|2002-06-01 00:00:...|\n|               17|                      2|         Pedals|6D24AC07-7A84-484...|2002-06-01 00:00:...|\n|               18|                      2|    Road Frames|5515F857-075B-4F9...|2002-06-01 00:00:...|\n|               19|                      2|        Saddles|049FFFA3-9D30-46D...|2002-06-01 00:00:...|\n|               20|                      2| Touring Frames|D2E3F1A8-56C4-4F3...|2002-06-01 00:00:...|\n+-----------------+-----------------------+---------------+--------------------+--------------------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 11, "cell_type": "code", "source": "from collections import namedtuple\nfrom pyspark.sql.types import *\n\nProductCategory = namedtuple(\"ProductCategory\", [\n  \"ProductCategoryID\",\n  \"ParentProductCategoryID\",\n  \"Name\",\n])\n\nschema=StructType([\n  StructField(\"ProductCategoryID\", IntegerType()),\n  StructField(\"ParentProductCategoryID\", IntegerType()),\n  StructField(\"Name\", StringType()),\n])\nnewProductCategoryDF = spark.createDataFrame([\n    (101, None, \"Computers\"),\n    (102, 101,  \"Laptop\"),\n    (103, 101,  \"Tablet\"),\n  ], schema)\nnewProductCategoryDF.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----------------+-----------------------+---------+\n|ProductCategoryID|ParentProductCategoryID|     Name|\n+-----------------+-----------------------+---------+\n|              101|                   null|Computers|\n|              102|                    101|   Laptop|\n|              103|                    101|   Tablet|\n+-----------------+-----------------------+---------+"}], "metadata": {"collapsed": false}}, {"execution_count": 12, "cell_type": "code", "source": "(newProductCategoryDF.repartition(1).write\n  .format(\"main.scala\")\n  .mode(\"append\")\n  .option(\"url\", jdbc_url_db)\n  .option(\"dbtable\", \"SalesLT.ProductCategory\")\n  .save())", "outputs": [{"output_type": "stream", "name": "stderr", "text": "An error occurred while calling o97.save.\n: java.lang.NoSuchMethodError: org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.tableExists(Ljava/sql/Connection;Lorg/apache/spark/sql/execution/datasources/jdbc/JDBCOptions;)Z\n\tat main.scala.DefaultSource.createRelation(DefaultSource.scala:43)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:518)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n\nTraceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/readwriter.py\", line 548, in save\n    self._jwrite.save()\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o97.save.\n: java.lang.NoSuchMethodError: org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.tableExists(Ljava/sql/Connection;Lorg/apache/spark/sql/execution/datasources/jdbc/JDBCOptions;)Z\n\tat main.scala.DefaultSource.createRelation(DefaultSource.scala:43)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:518)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n"}], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark3", "name": "pyspark3kernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python3", "name": "pyspark3", "codemirror_mode": {"version": 3, "name": "python"}}}}